import ollama


class OllamaNLG:
    def __init__(self, model: str = "mistral"):
        self.model = model

    def generate(
        self,
        response=None,
        user_text: str | None = None,
        introduce: bool = False
    ) -> str:
        """
        G√©n√®re une r√©ponse naturelle.
        - response : objet Response (si issu d'un skill)
        - user_text : texte brut utilisateur (fallback conversationnel)
        - introduce : indique si Mantis doit se pr√©senter
        """

        # üîπ Instructions syst√®me de base
        system_prompt = (
            "Tu es Mantis, une IA locale.\n"
            "Tu r√©ponds en fran√ßais, de mani√®re naturelle et concise.\n"
            "Tu utilises vous pour t'adresser √† l'utilisateur.\n"
            "Tu parles √† la premi√®re personne.\n"
        )

        # üîπ Pr√©sentation contr√¥l√©e par le Kernel
        if introduce:
            system_prompt += (
                "Pr√©sente-toi en disant explicitement : 'Bonjour, je suis Mantis'. Fais-le UNE SEULE FOIS.\n"
            )
        else:
            system_prompt += (
                "Ne te pr√©sente PAS.\n"
                "Ne dis PAS ton nom\n"
                "Ne dis PAS Bonjour ou Salut\n"
                "R√©ponds directement √† la demande\n"
            )

        # üîπ Construction du prompt
        if response is not None:
            prompt = (
                system_prompt
                + "\nType de r√©ponse : "
                + str(response.type)
                + "\nDonn√©es : "
                + str(response.data)
                + "\nR√©ponse :"
            )
        else:
            prompt = (
                system_prompt
                + "\nUtilisateur : "
                + str(user_text)
                + "\nMantis :"
            )

        result = ollama.generate(
            model=self.model,
            prompt=prompt
        )

        return result["response"].strip()
